{"cells":[{"cell_type":"code","execution_count":null,"id":"e847d1a2","metadata":{"id":"e847d1a2"},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from PIL import Image\n","\n","prnu_tens = transforms.ToTensor()\n","\n","def shuffle_split(data_list, n_train):\n","    idx_list = list(range(len(data_list)))\n","    np.random.shuffle(idx_list)\n","    train_idx_list = idx_list[:n_train]\n","    test_idx_list = idx_list[n_train:]\n","    return np.array(data_list)[train_idx_list], np.array(data_list)[test_idx_list]\n","\n","\n","def prepare_data_sets(real_dir, fake_dir, h_dict, test_only=False):\n","    real_path_list = [list(real_dir.glob('*.' + x)) for x in ['jpg', 'jpeg', 'png']]\n","    real_paths = [ele for ele in real_path_list if ele != []][0]\n","\n","    fake_path_list = [list(fake_dir.glob('*.' + x)) for x in ['jpg', 'jpeg', 'png']]\n","    fake_paths = [ele for ele in fake_path_list if ele != []][0]\n","\n","    if not test_only:\n","        n_train = h_dict[\"Train Size\"]\n","\n","        n_real = len(real_paths)\n","        n_fake = len(fake_paths)\n","        n = n_real\n","\n","        if n_real > n_fake:\n","\n","            idx_list = list(range(n_real))\n","            np.random.shuffle(idx_list)\n","            img_idx_list = idx_list[:n_fake]\n","            real_paths = [real_paths[i] for i in img_idx_list]\n","            n = n_fake\n","\n","        elif n_real < n_fake:\n","\n","            idx_list = list(range(n_fake))\n","            np.random.shuffle(idx_list)\n","            img_idx_list = idx_list[:n_real]\n","            fake_paths = [fake_paths[i] for i in img_idx_list]\n","            n = n_real\n","\n","        if n < n_train:\n","            raise Exception(f\"{n_train} images were requested for train, but there are only {n} images.\")\n","\n","        train_real_paths, test_real_paths = shuffle_split(real_paths, n_train)\n","        train_fake_paths, test_fake_paths = shuffle_split(fake_paths, n_train)\n","\n","        train_set = PreProcessData(train_real_paths, train_fake_paths, h_dict)\n","        test_set = PreProcessData(test_real_paths, test_fake_paths, h_dict)\n","\n","        file_dict = {\"Train Real\": train_real_paths,\n","                     \"Test Real\": test_real_paths,\n","                     \"Train Fake\": train_fake_paths,\n","                     \"Test Fake\": test_fake_paths}\n","\n","        return train_set, test_set, file_dict\n","\n","    else:\n","        test_set = PreProcessData(real_path_list, fake_path_list, h_dict)\n","\n","        file_dict = {\"Real\": real_path_list,\n","                     \"Fake\": fake_path_list}\n","\n","        return None, test_set, file_dict\n","\n","\n","def rescale_img(img):\n","    return (img - img.min()) / (img.max() - img.min() + 1e-8)\n","\n","\n","def load_pil_image(img_path, img_size=None):\n","    img = Image.open(img_path).convert('RGB')\n","\n","    if img_size is not None:\n","        w, h = img.size\n","        left = (w - img_size[1]) / 2\n","        top = (h - img_size[0]) / 2\n","        right = (w + img_size[1]) / 2\n","        bottom = (h + img_size[0]) / 2\n","\n","        img = img.crop((left, top, right, bottom))\n","\n","    return img\n","\n","def produce_fft(finger_npy):\n","    fft_f = np.fft.fft2(finger_npy - finger_npy.mean(), axes=(0, 1), norm='forward')\n","\n","    finger_spec = rescale_img(np.log(np.abs(fft_f)))\n","    finger_spec = np.fft.fftshift(finger_spec) ** 4\n","\n","    return finger_spec\n","\n","class PreProcessData(Dataset):\n","    def __init__(self, real_paths, fake_paths, hyper_pars,\n","                 demand_equal=True,\n","                 train_mode=True):\n","\n","        self.real_paths = real_paths\n","        self.fake_paths = fake_paths\n","        self.file_list = None\n","\n","        self.real_labels = None\n","        self.fake_labels = None\n","        self.label_list = None\n","\n","        self.crop_size = hyper_pars['Crop Size']\n","        self.batch_size = hyper_pars['Batch Size']\n","\n","        self.prepare_inputs(demand_equal)\n","        self.init_loader()\n","        self.train_mode = train_mode\n","\n","    def prepare_inputs(self, demand_equal):\n","\n","        n_real = len(self.real_paths)\n","        n_fake = len(self.fake_paths)\n","        n = n_real\n","\n","        if demand_equal:\n","            if n_real > n_fake:\n","                self.real_paths = self.real_paths[:n_fake]\n","                n = n_fake\n","\n","            elif n_real < n_fake:\n","                self.fake_paths = self.fake_paths[:n_real]\n","                n = n_real\n","\n","        self.real_labels = torch.zeros((len(self.real_paths),))\n","        self.fake_labels = torch.ones((len(self.fake_paths),))\n","\n","        self.file_list = np.array(list(self.real_paths) + list(self.fake_paths))\n","        self.label_list = torch.cat((self.real_labels, self.fake_labels), dim=0).type(torch.bool)\n","\n","    def init_loader(self):\n","        self.loader = DataLoader(self, batch_size=self.batch_size, shuffle=True, drop_last=False)\n","\n","    def get_loader(self):\n","        return self.loader\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.file_list[idx]\n","\n","        image = load_pil_image(img_path, self.crop_size)\n","        label = self.label_list[idx]\n","        image = np.array(image)\n","\n","        image = torch.tensor(image.transpose((2, 0, 1))).type(torch.float32).div(255)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"id":"7b28e122","metadata":{"id":"7b28e122"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}