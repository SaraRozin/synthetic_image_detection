{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","ROOT_PATH = '/content/drive/My Drive/synthetic_image_detection/rule_based/'.replace(\" \", \"\\\\\")\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","from IPython.display import display\n","from torch import optim, nn\n","import torch\n","from tqdm import tqdm\n","import seaborn as sns\n","%run {ROOT_PATH + \"Unet_model.ipynb\"}\n","%run {ROOT_PATH + 'utils.ipynb'}"],"metadata":{"id":"yDm8gFbwj4wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716159278405,"user_tz":-180,"elapsed":39813,"user":{"displayName":"synthetic image detection project","userId":"04585902416463197835"}},"outputId":"a0f63aeb-8956-433a-ced7-464aa457103a"},"id":"yDm8gFbwj4wb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import torch\n","import numpy as np\n","import sys\n","sys.path.append('/content/drive/My Drive/synthetic_image_detection/rule_based')\n","from dncnn.trainer import TrainerDnCNN, load_model\n","\n","def load_denoiser(device: str, trainable:bool=False)-> torch.nn.Module:\n","    print('device', device)\n","    sys.path.append('/content/drive/My Drive/synthetic_image_detection/rule_based')\n","\n","    denoiser_prnu_np = np.load(r\"/content/drive/My Drive/synthetic_image_detection/rule_based/dncnn/clean_real.npy\", allow_pickle=True)\n","\n","    trainer = load_model(TrainerDnCNN, r\"/content/drive/My Drive/synthetic_image_detection/rule_based/dncnn/parameters.pt\", device)\n","\n","    model = trainer.denoiser.to(device)\n","\n","    denoiser_prnu = torch.tensor(denoiser_prnu_np.transpose((2, 0, 1))).to(device).unsqueeze(0)\n","\n","    model.prnu = denoiser_prnu\n","\n","    if not trainable:\n","        model.eval()\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    else:\n","        for param in model.parameters():\n","            param.requires_grad = True\n","\n","    return model"],"metadata":{"id":"4Xk4i8x-jz1P"},"id":"4Xk4i8x-jz1P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","relu = nn.ReLU()\n","\n","\n","class FingerprintGenerator(nn.Module):\n","    def __init__(self, model_name, epoch=100, batch=64, is_trained=False):\n","        checkpoints_dir = r'/content/drive/My Drive/synthetic_image_detection/rule_based/checkpoints/'\n","        model_checkpoint_dir = Path(checkpoints_dir, model_name)\n","        check_existence(model_checkpoint_dir, False)\n","        with open(model_checkpoint_dir / \"train_hypers.pt\", 'rb') as pickle_file:\n","            hyperparams = pickle.load(pickle_file)\n","        if is_trained:\n","          self.load_stats(Path(model_checkpoint_dir, 'chk_{}.pt'.format(epoch)))\n","        hyperparams['Device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        hyperparams['Batch Size'] = batch\n","        super(FingerprintGenerator, self).__init__()\n","\n","        # Hyperparameters\n","        self.device = hyperparams['Device']\n","        self.init_lr = hyperparams['LR']\n","        self.ch_i = hyperparams['Inp. Channel']\n","        self.ch_o = hyperparams['Out. Channel']\n","        self.arch = hyperparams['Arch.']\n","        self.depth = hyperparams['Depth']\n","        self.concat = np.array(hyperparams['Concat'])\n","        self.m = hyperparams['Margin']\n","        self.batch_size = hyperparams['Batch Size']\n","        self.alpha = hyperparams['Alpha']\n","        try:\n","            self.boost = hyperparams['Boost']\n","        except:\n","            self.boost = False\n","\n","        self.train_loss = []\n","        self.train_corr_r = None\n","        self.train_corr_f = None\n","\n","        self.test_loss = []\n","        self.test_corr_r = []\n","        self.test_corr_f = []\n","        self.test_labels = []\n","\n","        self.noise_type = hyperparams['Noise Type']\n","        self.noise_std = hyperparams['Noise STD']\n","        self.noise_channel = hyperparams['Inp. Channel']\n","        self.crop_size = hyperparams['Crop Size']\n","\n","        d_h, n_h, d_w, n_w = calc_even_size(self.crop_size, self.depth)\n","        self.crop_size = (n_h - d_h, n_w - d_w)\n","        self.d_h, self.n_h = d_h, n_h\n","        self.d_w, self.n_w = d_w, n_w\n","\n","        # Model initialization\n","        self.noise = None\n","\n","        self.denoiser = load_denoiser(self.device)\n","        self.unet = Unet(self.device, self.ch_i, self.ch_o, self.arch, activ='leak', depth=self.depth, concat=self.concat).to(self.device)\n","        self.optimizer = optim.AdamW(self.unet.parameters(), lr=self.init_lr)\n","        self.loss_mse = nn.MSELoss()\n","\n","        self.init_train()\n","\n","    def norm_val(self, arr):\n","        return (arr - arr.mean((1, 2, 3)).view(-1, 1, 1, 1)) / (arr.std((1, 2, 3)).view(-1, 1, 1, 1) + 1e-8)\n","\n","    def init_train(self, n=1):\n","        self.noise = init_dummy(n, self.noise_type, self.crop_size, self.noise_channel)\n","        self.fingerprint = None\n","\n","    def prep_noise(self, var=-1):\n","        if var == -1:\n","            return self.noise + torch.randn_like(self.noise.detach()) * self.noise_std\n","        else:\n","            return self.noise + torch.randn_like(self.noise.detach()) * var\n","\n","    def corr_fun(self, out, target):\n","        # Pearson Correlation Coefficient (NNC(0,0))\n","        out = self.norm_val(out)\n","        target = self.norm_val(target)\n","\n","        return out * target\n","\n","\n","    def loss_contrast(self, correlations: np.array, labels: np.array) -> float:\n","        # labels: 0 for Real, 1 for Fake\n","        # -1 < correlations < 1\n","        # panish on the distanse from 0 for the correlations\n","        # between the real residual images and the generated fingerprint.\n","        real_loss = ~labs * torch.abs(corrs)\n","        # panish on the distanse from 1 for the correlations\n","        # between the fake residual images and the generated fingerprint.\n","        fake_loss = labs * (1 - torch.abs(corrs))\n","        loss = real_loss + fake_loss\n","        return loss\n","\n","\n","    # def loss_contrast(self, corrs, labs):\n","    #     # Label: 0 - Real, 1 - Fake\n","    #     real_loss = ~labs * torch.abs(corrs)\n","    #     fake_loss = labs * (1 - corrs)\n","    #     negetive_loss = -1 * torch.min(torch.tensor(0), corrs)\n","    #     loss = real_loss + fake_loss + negetive_loss\n","    #     return loss\n","\n","\n","    def train_step(self, images, labels):\n","\n","        images = images.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        self.unet.train()\n","        self.optimizer.zero_grad()\n","\n","        residuals = self.denoiser.denoise(images).detach()\n","        alpha = (1 - self.alpha) * torch.rand((len(images), 1, 1, 1)).to(self.device) + self.alpha\n","        residuals = alpha * residuals\n","\n","        f_mean = residuals[labels].mean(0, keepdims=True)\n","        r_mean = residuals[~labels].mean(0, keepdims=True)\n","        residuals = torch.cat((residuals, f_mean, r_mean), dim=0)\n","        dmy = self.prep_noise().to(self.device)\n","        out = self.unet(dmy).repeat(len(images) + 2, 1, 1, 1)\n","        corr = self.corr_fun(out, residuals)\n","\n","\n","        loss = self.loss_contrast(corr[:-2].mean((1, 2, 3)), labels).mean()\n","        if self.boost:\n","            corr_mean_d = torch.sqrt((corr[-2].mean() - corr[-1].mean()) ** 2)\n","            loss_b = relu(self.m - corr_mean_d)\n","\n","            loss += loss_b\n","            loss *= 0.5\n","\n","        gradients = [param.grad for param in self.unet.parameters() if param.grad is not None]\n","        loss.backward()\n","        gradients = [param.grad for param in self.unet.parameters() if param.grad is not None]\n","        self.optimizer.step()\n","\n","        if self.fingerprint is None:\n","            self.fingerprint = out[0:1].detach()\n","        else:\n","            self.fingerprint = self.fingerprint * 0.99 + out[0:1].detach() * (1 - 0.99)\n","\n","        corr = self.corr_fun(self.fingerprint.repeat(len(images), 1, 1, 1), residuals[:-2]).mean((1, 2, 3))\n","\n","        self.train_loss.append(loss.item())\n","\n","        if self.train_corr_r is None:\n","            self.train_corr_r = [corr[~labels].mean().item()]\n","            self.train_corr_f = [corr[labels].mean().item()]\n","        else:\n","            corr_r = corr[~labels]\n","            corr_f = corr[labels]\n","            self.train_corr_r.append(corr_r.mean().item())\n","            self.train_corr_f.append(corr_f.mean().item())\n","\n","\n","    def reset_test(self):\n","        self.test_corr_r = None\n","        self.test_corr_f = None\n","\n","        self.test_loss = []\n","        self.test_labels = []\n","\n","    def test_model(self, test_loader, custom_finger=None):\n","\n","        self.reset_test()\n","        self.calc_centers()\n","\n","        if custom_finger is None:\n","            fingerprint = self.fingerprint.to(self.device)\n","            fingerprint.repeat((self.batch_size, 1, 1, 1))\n","\n","\n","        else:\n","            if isinstance(custom_finger, np.ndarray):\n","                custom_finger = torch.Tensor(custom_finger.transpose((2, 0, 1))).type(torch.float32)\n","\n","            fingerprint = custom_finger.to(self.device)\n","            fingerprint = fingerprint.repeat((self.batch_size, 1, 1, 1))\n","\n","        with torch.no_grad():\n","            for images, labels in tqdm(test_loader, desc='Testing Model'):\n","                images = images.to(self.device)\n","                labels = labels.to(self.device)\n","                residuals = self.denoiser.denoise(images).float()\n","                corr = self.corr_fun(fingerprint, residuals)\n","                loss = self.loss_contrast(corr.mean((1, 2, 3)), labels)\n","                corr = corr.mean((1, 2, 3))\n","\n","                self.test_loss = self.test_loss + loss.tolist()\n","                self.test_labels = self.test_labels + labels.tolist()\n","\n","                if self.test_corr_r is None:\n","                    self.test_corr_r = corr[~labels].cpu().numpy()\n","                    self.test_corr_f = corr[labels].cpu().numpy()\n","                else:\n","                    self.test_corr_r = np.append(self.test_corr_r, corr[~labels].cpu().numpy(), axis=0)\n","                    self.test_corr_f = np.append(self.test_corr_f, corr[labels].cpu().numpy(), axis=0)\n","\n","    def produce_fingerprint(self, np=True):\n","        with torch.no_grad():\n","            out = self.fingerprint[0]\n","\n","        if np:\n","            return out.cpu().numpy().transpose((1, 2, 0))\n","        else:\n","            return out\n","\n","    def plot_loss(self, train=True):\n","        plt.figure(figsize=(10, 6))\n","\n","        if train:\n","            plt.scatter(np.arange(1, len(self.train_loss) + 1), self.train_loss, s=3, label='Loss', c='g')\n","            plt.xlabel('Batch Index')\n","            plt.ylabel('Mean Sample Loss')\n","            plt.title('Train Loss')\n","\n","        else:\n","            self.test_labels = np.array(self.test_labels)\n","            colors = np.array([(1., 0., 0.)] * len(self.test_labels))\n","            colors[self.test_labels == 0] = (0., 1., 0.)\n","\n","            plt.scatter(np.arange(1, len(self.test_loss) + 1), self.test_loss, s=3, label='Loss', c=colors)\n","            plt.xlabel('Label Index')\n","            plt.ylabel('Sample Loss')\n","            plt.title('Test Loss')\n","\n","        plt.grid(True)\n","        plt.ylim([0., 1.0])\n","        plt.legend(fontsize=12)\n","        plt.tight_layout()\n","\n","        plt.show()\n","\n","    def show_fingerprint(self):\n","        finger = self.produce_fingerprint()\n","        finger = 0.5 * finger + 0.5\n","\n","        plt.figure(figsize=(4, 4))\n","\n","        plt.imshow(finger)\n","        plt.axis(False)\n","        plt.title('Fingerprint')\n","\n","        plt.show()\n","\n","        dct_finger = produce_spectrum(finger)\n","        dct_finger = (dct_finger - dct_finger.min()) / (dct_finger.max() - dct_finger.min())\n","\n","        plt.figure(figsize=(4, 4))\n","\n","        plt.imshow(dct_finger, 'bone')\n","        plt.axis(False)\n","        plt.title('Fingerprint FFT')\n","\n","        plt.show()\n","\n","\n","    def plot_corr(self, train=True):\n","\n","        plt.figure(figsize=(10, 6))\n","\n","        if train:\n","\n","            plt.scatter(np.arange(len(self.train_corr_r)), self.train_corr_r, s=3,\n","                        label='Real Corr.', c='g')\n","            plt.scatter(np.arange(len(self.train_corr_f)), self.train_corr_f, s=3,\n","                        label='Fake Corr.', c='r')\n","\n","            plt.xlabel('Batch Index')\n","            plt.ylabel('Mean Sample Corr.')\n","            plt.title('Train Correlation')\n","\n","        else:\n","\n","            plt.scatter(np.arange(1, len(self.test_corr_r) + 1), self.test_corr_r, s=3, label='Real Corr.', c='g')\n","            plt.scatter(np.arange(1, len(self.test_corr_f) + 1), self.test_corr_f, s=3, label='Fake Corr.', c='r')\n","            plt.xlabel('Label Index')\n","            plt.title('Test Correlation')\n","            plt.ylabel('Sample Corr.')\n","\n","        plt.grid(True)\n","        plt.legend(fontsize=12)\n","\n","        plt.show()\n","        plt.close()\n","\n","\n","\n","    def view_corr(self, train=True):\n","        plt.figure(figsize=(10, 6))\n","\n","        if train:\n","\n","            plt.scatter(np.arange(len(self.train_corr_r)), self.train_corr_r, s=3,\n","                        label='Real Corr.', c='g')\n","            plt.scatter(np.arange(len(self.train_corr_f)), self.train_corr_f, s=3,\n","                        label='Fake Corr.', c='r')\n","\n","            plt.xlabel('Batch Index')\n","            plt.ylabel('Mean Sample Corr.')\n","            plt.title('Train Correlation')\n","\n","        else:\n","\n","            plt.scatter(np.arange(1, len(self.test_corr_r) + 1), self.test_corr_r, s=3, label='Real Corr.', c='g')\n","            plt.scatter(np.arange(1, len(self.test_corr_f) + 1), self.test_corr_f, s=3, label='Fake Corr.', c='r')\n","            plt.xlabel('Label Index')\n","            plt.title('Test Correlation')\n","            plt.ylabel('Sample Corr.')\n","\n","        plt.legend()\n","        plt.grid(True)\n","\n","        # Use display instead of plt.show()\n","        display(plt.gcf())\n","\n","\n","    def calc_centers(self):\n","        self.mu_real = np.mean(self.train_corr_r[-20:])\n","        self.mu_fake = np.mean(self.train_corr_f[-20:])\n","\n","    def calc_distance(self):\n","        dist_real = distance(self.test_corr_r, self.mu_real, self.mu_fake)\n","        dist_fake = distance(self.test_corr_f, self.mu_real, self.mu_fake)\n","\n","        return dist_fake, dist_real\n","\n","    def calc_accuracy(self, val=None, print_res=True):\n","\n","        if val is not None:\n","            dist = distance(val, self.mu_real, self.mu_fake)\n","            cls = np.argmin(dist, axis=1)\n","\n","            return dist[0], cls[0]\n","\n","        else:\n","            # Real - 0, Fake - 1\n","            dist_real = distance(self.test_corr_r, self.mu_real, self.mu_fake)\n","            dist_fake = distance(self.test_corr_f, self.mu_real, self.mu_fake)\n","\n","            class_real = np.argmin(dist_real, axis=1) == 0\n","            class_fake = np.argmin(dist_fake, axis=1) == 1\n","\n","            acc_real = class_real.sum() / len(class_real)\n","            acc_fake = class_fake.sum() / len(class_fake)\n","\n","            if print_res:\n","                print(\"Accuracy by cluster means:\")\n","                print(f\" Real samples: {acc_real:.2f}\")\n","                print(f\" Fake samples: {acc_fake:.2f}\")\n","                print(f\" All samples: {(acc_real + acc_fake) / 2.:.2f}\")\n","\n","            return acc_fake, acc_real\n","\n","    def show_prnu_density(self, title=None):\n","\n","        corr_r = self.test_corr_r\n","        corr_f = self.test_corr_f\n","\n","        fig, ax = plt.subplots(figsize=(4, 4))\n","        for val, data_type, mu in zip([corr_r, corr_f], [\"Real\", \"Fake\"], [self.mu_real, self.mu_fake]):\n","            hist = np.histogram(val, bins=100)\n","            width = (hist[1][-1] - hist[1][0]) / 100\n","            ax.bar(hist[1][1:], hist[0], width, alpha=0.5, label=f'{data_type}')\n","\n","            ax.axvline(x=mu, ymin=0, ymax=np.max(hist[0]), linestyle=\"--\", color='k',\n","                       label=r'$\\mu_{' + f'{data_type}' + '}$')\n","\n","        ax.set_ylabel('Count')\n","        ax.set_xlabel(r'$\\rho$')\n","        ax.legend()\n","        ax.grid()\n","        if title is not None:\n","            plt.title(title)\n","\n","        fig.tight_layout()\n","        fig.show()\n","\n","\n","    def save_stats(self, path):\n","        self.calc_centers()\n","\n","        data_dict = {'Fingerprint': self.fingerprint,\n","                     'Train Real': self.train_corr_r,\n","                     'Train Fake': self.train_corr_f,\n","                     'Loss': self.train_loss}\n","\n","        torch.save(data_dict, path)\n","\n","    def load_stats(self, path):\n","        if self.device.type == 'cpu':\n","            data_dict = torch.load(path, map_location=torch.device('cpu'))\n","        else:\n","            data_dict = torch.load(path)\n","\n","        self.train_loss = data_dict['Loss']\n","        self.train_corr_r = data_dict['Train Real']\n","        self.train_corr_f = data_dict['Train Fake']\n","        self.fingerprint = data_dict['Fingerprint']\n","\n","    def denoise_image(self, img_path):\n","        image = load_pil_image(img_path, self.crop_size)\n","        image = np.array(image)\n","\n","        image = torch.tensor(image.transpose((2, 0, 1))).type(torch.float32).div(255)\n","        image = image.unsqueeze(0)\n","\n","        residuals = self.denoiser.denoise(image.to(self.device)).detach()\n","        # alpha = (1 - self.alpha) * torch.rand((len(image), 1, 1, 1)).to(self.device) + self.alpha\n","        # residuals = alpha * residuals\n","        return residuals\n","\n","\n","    def get_correlation(self, img_path):\n","        image = load_pil_image(img_path, self.crop_size)\n","        image = np.array(image)\n","\n","        image = torch.tensor(image.transpose((2, 0, 1))).type(torch.float32).div(255)\n","        image = image.unsqueeze(0)  # Adding batch size of 1\n","\n","        residuals = self.denoiser.denoise(image.to(self.device)).detach()\n","        # alpha = (1 - self.alpha) * torch.rand((len(image), 1, 1, 1)).to(self.device) + self.alpha\n","        # residuals = alpha * residuals\n","        corr = self.corr_fun(self.fingerprint, residuals)\n","        corr = corr.mean((1, 2, 3))\n","        return corr\n","\n","\n","    def show_residul(self, image):\n","        image = 0.5 * image + 0.5\n","        image = image.cpu().detach().numpy()  # Move tensor to CPU, detach, and convert to numpy\n","        image = image.squeeze().transpose((1, 2, 0))  # Reshape the image for display\n","        plt.figure(figsize=(4, 4))\n","        plt.imshow(image)\n","        plt.axis('off')\n","        plt.title('Residual')\n","        plt.show()\n","        # Assuming produce_spectrum works with numpy arrays\n","        dct_finger = produce_spectrum(image)\n","        dct_finger = (dct_finger - dct_finger.min()) / (dct_finger.max() - dct_finger.min())\n","        plt.figure(figsize=(4, 4))\n","        plt.imshow(dct_finger, 'bone')\n","        plt.axis('off')\n","        plt.title('Residual FFT')\n","        plt.show()\n","\n","\n","def init_dummy(bs, noise_type, img_dims, ch_n, var=0.1):\n","    if noise_type == 'uniform':\n","        img = var * torch.rand((bs, ch_n, img_dims[0], img_dims[1]))\n","    elif noise_type == 'normal':\n","        img = var * torch.randn((bs, ch_n, img_dims[0], img_dims[1]))\n","    elif noise_type == 'mesh':\n","        assert ch_n == 2\n","        X, Y = np.meshgrid(np.arange(0, img_dims[1]) / float(img_dims[1] - 1),\n","                           np.arange(0, img_dims[0]) / float(img_dims[0] - 1))\n","        meshgrid = np.concatenate([X[None, :], Y[None, :]])\n","        img = torch.tensor(meshgrid).unsqueeze(0).type(torch.float)\n","\n","    elif noise_type == 'special':\n","        X, Y = np.meshgrid(np.arange(0, img_dims[1]) / float(img_dims[1] - 1),\n","                           np.arange(0, img_dims[0]) / float(img_dims[0] - 1))\n","        meshgrid = np.concatenate([X[None, :], Y[None, :]])\n","        img = torch.tensor(meshgrid).unsqueeze(0).type(torch.float)\n","        img = torch.cat((img, torch.ones((1, 1, img_dims[0], img_dims[1]))), dim=1)\n","    return img\n","\n","\n","def distance(arr, mu_a, mu_b):\n","    dist_arr2a = np.sqrt(((arr - mu_a) ** 2)).reshape((-1, 1))\n","    dist_arr2b = np.sqrt(((arr - mu_b) ** 2)).reshape((-1, 1))\n","    return np.concatenate((dist_arr2a, dist_arr2b), axis=1)"],"metadata":{"id":"aDgzgrDimNih"},"id":"aDgzgrDimNih","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h0nCB3DYaxQ0"},"id":"h0nCB3DYaxQ0","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}